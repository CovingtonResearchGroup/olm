--- docs/conf.py	(original)
+++ docs/conf.py	(refactored)
@@ -55,8 +55,8 @@
 master_doc = 'index'
 
 # General information about the project.
-project = u'olm'
-copyright = u'2014,2015 Matthew Covington'
+project = 'olm'
+copyright = '2014,2015 Matthew Covington'
 
 # The version info for the project you're documenting, acts as replacement for
 # |version| and |release|, also used in various other places throughout the
@@ -205,8 +205,8 @@
 # Grouping the document tree into LaTeX files. List of tuples
 # (source start file, target name, title, author, documentclass [howto/manual]).
 latex_documents = [
-  ('index', '.tex', u'. Documentation',
-   u'Author', 'manual'),
+  ('index', '.tex', '. Documentation',
+   'Author', 'manual'),
 ]
 
 # The name of an image file (relative to this directory) to place at the top of
@@ -235,8 +235,8 @@
 # One entry per manual page. List of tuples
 # (source start file, name, description, authors, manual section).
 man_pages = [
-    ('index', '', u'. Documentation',
-     [u'Author'], 1)
+    ('index', '', '. Documentation',
+     ['Author'], 1)
 ]
 
 # If true, show URL addresses after external links.
@@ -249,8 +249,8 @@
 # (source start file, target name, title, author,
 #  dir menu entry, description, category)
 texinfo_documents = [
-  ('index', '', u'. Documentation',
-   u'Author', '', 'One line description of project.',
+  ('index', '', '. Documentation',
+   'Author', '', 'One line description of project.',
    'Miscellaneous'),
 ]
 
@@ -270,10 +270,10 @@
 # -- Options for Epub output ---------------------------------------------------
 
 # Bibliographic Dublin Core info.
-epub_title = u'.'
-epub_author = u'Author'
-epub_publisher = u'Author'
-epub_copyright = u'2013, Author'
+epub_title = '.'
+epub_author = 'Author'
+epub_publisher = 'Author'
+epub_copyright = '2013, Author'
 
 # The language of the text. It defaults to the language option
 # or en if the language is not set.
--- examples/example-plots.py	(original)
+++ examples/example-plots.py	(refactored)
@@ -3,7 +3,7 @@
 #make some initial plots of the data using the code below.  This
 #provides only a short example of what can be done.
 
-import cPickle as pickle
+import pickle as pickle
 from pylab import *
 
 #Read in the Pandas Panel that contains the data for one site from the pickled output file.
@@ -17,9 +17,9 @@
 data = data.join(data_phrq, rsuffix='_phrq')
 
 #These are the column labels we have available
-print "Available data:"
-print(data.columns.values)
-print " "
+print("Available data:")
+print((data.columns.values))
+print(" ")
 #Grab temperature data
 temp = data['Temperature, water']
 #Plot it out as a time series
@@ -38,9 +38,9 @@
 ylabel('Alkalinity (as mg/L CaCO3)')
 show()
 #We can also do some simple stats
-print "Alkalinity stats:"
-print(alk.describe())
-print " "
+print("Alkalinity stats:")
+print((alk.describe()))
+print(" ")
 
 #Plot SI Calcite vs. Discharge
 figure()
@@ -58,8 +58,8 @@
 lat = site_data['dec_lat_va']
 longitude = site_data['dec_long_va']
 
-print 'Station name = ', name
-print 'Latitude = ', lat
-print 'Longitude = ', longitude
-print 'Time Zone = ', timezone
-print 'Drainage basin area = ', area
+print('Station name = ', name)
+print('Latitude = ', lat)
+print('Longitude = ', longitude)
+print('Time Zone = ', timezone)
+print('Drainage basin area = ', area)
--- olm/__init__.py	(original)
+++ olm/__init__.py	(refactored)
@@ -1,3 +1,3 @@
-from general import *
+from .general import *
 
 __all__=['calcite','general']
--- olm/calcite.py	(original)
+++ olm/calcite.py	(refactored)
@@ -4,7 +4,7 @@
 
 import numpy as np
 import pandas
-from general import *
+from .general import *
 from scipy.optimize import brentq#, fminbound
 from scipy.optimize import fsolve
 from scipy.interpolate import LinearNDInterpolator, interp1d
@@ -570,11 +570,11 @@
             elif method=='franci':
                 R = pwpRateFranci(a_Ca=a_Ca, a_H2CO3s=a_H2CO3s, a_H=a_H, a_HCO3=a_HCO3, T_K=T_K,PCO2=PCO2_in)
             else:
-                print "method must be set to 'theory', 'pascal', or 'franci'"
+                print("method must be set to 'theory', 'pascal', or 'franci'")
                 return -1
             return R
         else:
-            print "Not all necessary ions are present in the solution object."
+            print("Not all necessary ions are present in the solution object.")
             return -1 
     is_series = (type(sol)==pandas.core.series.Series)
     #or (type(sol)==pandas.core.series.TimeSeries)
@@ -1212,16 +1212,16 @@
     T_min = 5.
     T_max = 25.
     if logPCO2<logPCO2_min:
-        print "Warning! Low PCO2 outside of interpolation range is set to minimum from table."
+        print("Warning! Low PCO2 outside of interpolation range is set to minimum from table.")
         logPCO2=logPCO2_min
     if logPCO2>logPCO2_max:
-        print "Warning! High PCO2 outside of interpolation range is set to maximum from table."
+        print("Warning! High PCO2 outside of interpolation range is set to maximum from table.")
         logPCO2=logPCO2_max
     if T_C<T_min:
-        print "Warning! Low temp outside of interpolation range is set to minimum from table."
+        print("Warning! Low temp outside of interpolation range is set to minimum from table.")
         T_C=T_min
     if T_C>T_max:
-        print "Warning! High temp outside of interpolation range is set to maximum from table."
+        print("Warning! High temp outside of interpolation range is set to maximum from table.")
         T_C = T_max
     palmer_k1, palmer_C_Cs_T, palmer_n = interp_funcs
     k1 = palmer_k1(T_C, logPCO2)
--- olm/general.py	(original)
+++ olm/general.py	(refactored)
@@ -135,17 +135,17 @@
                 if (properties[ion]['charge']==0):
                     activity_coef = neutralGamma(I)
                     return activity_coef
-            print "Radius of " + ion + " not available."
+            print("Radius of " + ion + " not available.")
             return None
         if 'charge' in properties[ion]:
             charge = properties[ion]['charge']
         else:
-            print "Charge of " + ion + " not available."
+            print("Charge of " + ion + " not available.")
             return None
         activity_coef = DebyeHuckel(I,charge,radius,T=T_C)
         return activity_coef
     else:
-        print "Ion not in properties list: " + ion
+        print("Ion not in properties list: " + ion)
         return None
     
 def HardnessFromCond(cond, T_C=None):
@@ -255,7 +255,7 @@
         mgL = (1000. * mw)*molL
         return mgL
     else:
-        print "Ion +", ion, " not in properties."
+        print("Ion +", ion, " not in properties.")
         return -1
 
 def mgL_to_molL(mgL, ion):
@@ -279,7 +279,7 @@
         molL = mgL/(1000.*mw)
         return molL
     else:
-        print "Ion +", ion, " not in properties."
+        print("Ion +", ion, " not in properties.")
         return -1
 
 def mmolL_to_meqL(mmolL, ion):
@@ -303,7 +303,7 @@
         meqL = mmolL*charge
         return meqL
     else:
-        print "Ion +", ion, " not in properties."
+        print("Ion +", ion, " not in properties.")
         return -1
 
 def molL_to_meqL(molL, ion):
@@ -371,9 +371,9 @@
                 if len(units) == 1:
                     units = [units[0]] * len(constituents)
                 else:
-                    print "Units must either contain the same number of items as constituents or only contain one item."
+                    print("Units must either contain the same number of items as constituents or only contain one item.")
             else:
-                print "Units must either be a list or a string."
+                print("Units must either be a list or a string.")
         if len(constituents) == len(concentrations):
             self.ions = {}
             for i, ion in enumerate(constituents):
@@ -397,7 +397,7 @@
                     ion_dict['meq'] = mmolL_to_meqL(ion_dict['conc_mmol'], ion)
                     self.ions[ion] = ion_dict
                 else:
-                    print "Units for " + ion + " are incorrect, must either be mg/L, mmol/L, or mol/L"
+                    print("Units for " + ion + " are incorrect, must either be mg/L, mmol/L, or mol/L")
             if T_units == 'C':
                 self.T_C = np.float(T)
                 self.T_K = CtoK(self.T_C)
@@ -405,20 +405,20 @@
                 self.T_K = np.float(T)
                 self.T_C = KtoC(self.T_K)
             else:
-                print "T_units must equal either C or K."
+                print("T_units must equal either C or K.")
             if cond != None:
                 self.cond = np.float(cond)
             if pH != None:
                 self.pH = np.float(pH)
             #loop through ions and calculate activities
-            for key, ion_dict in self.ions.iteritems():
+            for key, ion_dict in self.ions.items():
                 if ('conc_mol' in ion_dict):
                     gamma = self.activity_coef(key) #estimates ionic strength
                     if gamma != None:
                         activity = gamma*ion_dict['conc_mol']
                         self.ions[key]['activity'] = activity
         else:
-            print "Length of constituents and concentrations lists must be equal."
+            print("Length of constituents and concentrations lists must be equal.")
 
     def I(self):
         """
@@ -453,7 +453,7 @@
                 if 'conc_mol' in self.ions[metal]:
                     metals_conc += self.ions[metal]['conc_mol']
                 else:
-                    print "No known " + metal + " concentration in mol/L."
+                    print("No known " + metal + " concentration in mol/L.")
         I = 3.0 * metals_conc
         return I
 
@@ -569,17 +569,17 @@
                     if (properties[ion]['charge']==0):
                         activity_coef = neutralGamma(I)
                         return activity_coef
-                print "Radius of " + ion + " not available."
+                print("Radius of " + ion + " not available.")
                 return None
             if 'charge' in properties[ion]:
                 charge = properties[ion]['charge']
             else:
-                print "Charge of " + ion + " not available."
+                print("Charge of " + ion + " not available.")
                 return None
             activity_coef = DebyeHuckel(I,charge,radius,T=self.T_C)
             return activity_coef
         else:
-            print "Ion not in properties list:" + ion
+            print("Ion not in properties list:" + ion)
             return None
 
     def charge_balance(self, units='%'):
@@ -605,6 +605,6 @@
         elif units=='%':
             return numerator/denominator*100.
         else:
-            print "Invalid value for units: ", units
+            print("Invalid value for units: ", units)
             return
         
--- olm/USGS/DataRetrieval.py	(original)
+++ olm/USGS/DataRetrieval.py	(refactored)
@@ -5,8 +5,8 @@
 
 from lxml import etree
 import lxml.html
-import urllib, requests #could eventually rework to use only requests
-from StringIO import StringIO
+import urllib.request, urllib.parse, urllib.error, requests #could eventually rework to use only requests
+from io import StringIO
 from pandas import read_csv, DataFrame, to_datetime
 #import requests
 #import os
@@ -122,7 +122,7 @@
     BASEURL = 'https://waterservices.usgs.gov/nwis/site/?site='
     queryURL = BASEURL + sitenum + '&siteOutput=expanded'
     #Need to skip header, which is hopefully uniform across USGS queries
-    skiprows = range(0,59)
+    skiprows = list(range(0,59))
     skiprows.append(60)
     siteDF = read_csv(queryURL, sep='\t', skiprows=skiprows)
     siteDF = siteDF.ix[0] #change axis so that we only have key-data pairs
@@ -149,7 +149,7 @@
     #add mime type
     queryText += '&mimeType=xml'
     #convert query string to url special characters
-    queryText = urllib.quote(queryText, safe="/&=:?")
+    queryText = urllib.parse.quote(queryText, safe="/&=:?")
     return queryText
     
  
@@ -190,7 +190,7 @@
         r = requests.get(query_html)
         qtree = etree.parse(StringIO(r.content))
     except IOError:
-        print "Problem retrieving discharge value (IOError)."
+        print("Problem retrieving discharge value (IOError).")
         return -1
     #parse xml file to pull out discharge and quality code
     root = qtree.getroot()
@@ -258,7 +258,7 @@
         r = requests.get(query_html)
         qtree = etree.parse(StringIO(r.content))
     except IOError:
-        print "Problem retrieving discharge value (IOError)."
+        print("Problem retrieving discharge value (IOError).")
         return -1
     #parse xml file to pull out discharge and quality code
     root = qtree.getroot()
--- olm/USGS/PhreeqcPandas.py	(original)
+++ olm/USGS/PhreeqcPandas.py	(refactored)
@@ -6,7 +6,7 @@
 
 from numpy import isnan,size
 from pandas import DataFrame
-import cPickle as pickle
+import pickle as pickle
 import os, subprocess, xlrd, sys
 from olm.USGS.loadWaterQualityData import loadSiteListData
 from olm.general import molL_to_mgL
@@ -153,7 +153,7 @@
                     try:
                         os.makedirs(site_dir)
                     except os.error:
-                        print ("Problem creating location directory: " + sampledir)
+                        print(("Problem creating location directory: " + sampledir))
                         return -1
         if force_balance=='':
             phreeqc_file_name = os.path.join(site_dir, datetext+'.phrq')
@@ -167,7 +167,7 @@
         #run phreeqc on the input file just created
         phreeqcOutputFile = phreeqc_file_name[:-4]+'out'
         retcode = subprocess.call([os.path.join(PHREEQC_PATH,'phreeqc'), phreeqc_file_name, phreeqcOutputFile, DATABASE_FILE])
-        print "PHREEQC return code = ",retcode
+        print("PHREEQC return code = ",retcode)
         if retcode == 0:
             simulationDict = readPhreeqcOutput(phreeqcOutputFile)
         else: 
@@ -182,8 +182,8 @@
                 if (simulationDict != None):
                     #Retrieve charge balance error
                     balance_eq = float(simulationDict['Electrical balance'])
-                    print "balance_eq=", balance_eq
-                    if simulationDict.has_key('Alkalinity'):
+                    print("balance_eq=", balance_eq)
+                    if 'Alkalinity' in simulationDict:
                         alk = float(simulationDict['Alkalinity'])
                     else:
                         #Note: Some samples seem to have zero measured alkalinity
@@ -191,17 +191,17 @@
                         # convergence doesn't happen or we should quit trying.
                         # Perhaps negative alkalinity??
                         alk = 0.0
-                    print "alk=", alk
+                    print("alk=", alk)
                     New_alk_molL = alk + balance_eq#attempt to stabilize this using a factor of 0.9. Otherwise it seems to overshoot. this didn't work. 
                     if New_alk_molL < 0:#PHREEQC won't take negative alkalinity inputs
                         New_alk_molL = alk*0.5
-                    print "new alk=", New_alk_molL
+                    print("new alk=", New_alk_molL)
                     New_alk_mgL = 0.5*molL_to_mgL(New_alk_molL, 'CaCO3')#PHREEQC wants it as 0.5CaCO3
-                    print "new alk mg/L=", New_alk_mgL
+                    print("new alk mg/L=", New_alk_mgL)
                     sample_row['Alkalinity'] = New_alk_mgL
                     writePhreeqcInput(sample_row, phreeqc_file_name, phreeqcDict=phreeqcDict, datetext=datetext)
                     retcode = subprocess.call([os.path.join(PHREEQC_PATH,'phreeqc'), phreeqc_file_name, phreeqcOutputFile, DATABASE_FILE])
-                    print "PHREEQC return code = ",retcode
+                    print("PHREEQC return code = ",retcode)
                     if retcode == 0:
                         simulationDict = readPhreeqcOutput(phreeqcOutputFile)
                         number_of_iterations = number_of_iterations + 1
@@ -234,8 +234,8 @@
     #create dataframe from phreeqc outputs
     phreeqc_df = DataFrame(phreeqc_data_list, index=output_dates, dtype='float64')
     if force_balance=='Alk':
-        print "Number of converged alkalinity forced charge balance cases = ", num_converged
-        print "Total number of samples = ", total_num
+        print("Number of converged alkalinity forced charge balance cases = ", num_converged)
+        print("Total number of samples = ", total_num)
     return phreeqc_df
 
 def writePhreeqcInput(sample_row, phreeqc_file_name, phreeqcDict=default_phreeqc_to_WQX_translation, datetext='', charge=None):
@@ -263,22 +263,22 @@
     """
     try:
         phreeqc_input_file = open(phreeqc_file_name, 'w')
-        print >>phreeqc_input_file, 'SOLUTION ' + ' on ' + datetext
-        print >>phreeqc_input_file, 'units mg/l'
+        print('SOLUTION ' + ' on ' + datetext, file=phreeqc_input_file)
+        print('units mg/l', file=phreeqc_input_file)
         #loop through all characteristics that should be included in PHREEQC analysis
-        for characteristic in phreeqcDict.iterkeys():
+        for characteristic in phreeqcDict.keys():
             #Check to see if this characteristic is in our panel
-            if characteristic in sample_row.keys():
+            if characteristic in list(sample_row.keys()):
                 #Check for cases with duplicate dates and no time information
                 #In this case, get only first sample on that date
                 if size(sample_row.shape)>1 and min(sample_row.shape)>1:
                     sample_row = sample_row.ix[0]
                 if not isnan(sample_row[characteristic] ):
                     if phreeqcDict[characteristic]==charge:
-                        print >>phreeqc_input_file, phreeqcDict[characteristic] +' ' + str(sample_row[characteristic])+ ' '+'charge'
+                        print(phreeqcDict[characteristic] +' ' + str(sample_row[characteristic])+ ' '+'charge', file=phreeqc_input_file)
                     else:
-                        print >>phreeqc_input_file, phreeqcDict[characteristic] +' ' + str(sample_row[characteristic])
-        print >>phreeqc_input_file, "END"
+                        print(phreeqcDict[characteristic] +' ' + str(sample_row[characteristic]), file=phreeqc_input_file)
+        print("END", file=phreeqc_input_file)
         phreeqc_input_file.close()
         return 1
     except IOError:
@@ -319,8 +319,8 @@
     sitesDict = loadSiteListData(regEx=regEx, processedSitesDir = sitesDir)
     if process_regular:
         #Run PHREEQC on sites without forced charge balance
-        for site, site_panel in sitesDict.iteritems():
-            print("Processing "+site+" in PHREEQC")
+        for site, site_panel in sitesDict.items():
+            print(("Processing "+site+" in PHREEQC"))
             sitedf = processPanel(site_panel, os.path.join(sitesDir,site), PHREEQC_PATH=PHREEQC_PATH, DATABASE_FILE=DATABASE_FILE, phreeqcDict=phreeqcDict)
             phreeqc_site_file = os.path.join(sitesDir,site,site+'-PHREEQC.pkl')
             try:
@@ -330,7 +330,7 @@
                 print('Problem writing out PHREEQC data file.')
     if bracket_charge_balance:
         #Run PHREEQC using bracketed charge balance for sites
-        for site, site_panel in sitesDict.iteritems():               
+        for site, site_panel in sitesDict.items():               
             #Force balance on Calcium
             phreeqc_df_ca = processPanel(site_panel, os.path.join(sitesDir,site), PHREEQC_PATH, DATABASE_FILE, force_balance='Ca')               
             phreeqc_site_file_ca = os.path.join(sitesDir,site,site+'-PHREEQC-Ca.pkl')
--- olm/USGS/WQXtoPandas.py	(original)
+++ olm/USGS/WQXtoPandas.py	(refactored)
@@ -7,16 +7,16 @@
 import sys,xlrd,os,subprocess,string,requests
 from glob import glob
 from math import ceil
-import cPickle as pickle
+import pickle as pickle
 from lxml import etree
 from pandas import DataFrame, Panel, to_datetime, Series, concat
 from olm.USGS.PhreeqcPandas import processPanel
 
 #import functions from olm package
-from siteListExtraction import extractSitesFromXML
-from siteListExtraction import extractSitesFromText
-from DataRetrieval import querySiteList, GetDailyDischarge, GetSiteData
-from dataSlice import extractValues
+from .siteListExtraction import extractSitesFromXML
+from .siteListExtraction import extractSitesFromText
+from .DataRetrieval import querySiteList, GetDailyDischarge, GetSiteData
+from .dataSlice import extractValues
 
 def WQXtoPandas(xmlLocation, charDict, outputPath='.', fromFile=False, outputDirName='Processed-Sites', RUN_PHREEQC=False, PHREEQC_PATH='/home/mcoving/phreeqc-2.18.0/bin/', DATABASE_FILE='/home/mcoving/phreeqc-2.18.0/database/phreeqc.dat', LOG_FILE = 'Result.log', START_FILE = None, splittag='',bracket_charge_balance=False):
     """
@@ -69,12 +69,12 @@
         #Check to see if output directory exists
         absOutputDirPath = os.path.abspath(outputPath)
         sitesdir = os.path.join(absOutputDirPath, outputDirName)
-        print "sitesdir",sitesdir
+        print("sitesdir",sitesdir)
         if not(os.path.exists(sitesdir)):
             try:
                 os.makedirs(sitesdir)
             except os.error:
-                print("Problem creating output directory. Check output path name: "+outputPath)
+                print(("Problem creating output directory. Check output path name: "+outputPath))
                 return -1
         #create xml tree
         if fromFile:
@@ -86,7 +86,7 @@
             if ( os.path.isfile(xmlSaveFile) ):
                 goodAnswer = False
                 while not(goodAnswer):
-                    answer = raw_input("An xml file ("+ xmlSaveFile+ ") already exists.  \n Use this instead of html query (y or n)?")
+                    answer = input("An xml file ("+ xmlSaveFile+ ") already exists.  \n Use this instead of html query (y or n)?")
                     if (answer.startswith('y')):
                         #read from file
                         wqxtree = etree.ElementTree(file=xmlSaveFile)
@@ -99,18 +99,18 @@
                 queryXML = True
             #If we don't have a matching xml file, or we want to obtain a new one, then get the new xml
             if (queryXML):
-                print "Obtaining xml file from USGS NWIS using html query..."
+                print("Obtaining xml file from USGS NWIS using html query...")
                 #parse from html query                
                 r = requests.get(xmlLocation)
                 #write to xml file          
                 try:
                     #write xml to file
                     xmlFile = open(xmlSaveFile, 'w')
-                    print >>xmlFile, r.text
+                    print(r.text, file=xmlFile)
                     xmlFile.close()
                     wqxtree = etree.ElementTree(file = xmlSaveFile)
                 except IOError:
-                    print ("Problem writing to xml file to store html query: " + xmlSaveFile)
+                    print(("Problem writing to xml file to store html query: " + xmlSaveFile))
                     return -1
         #begin parsing XML tree
         root = wqxtree.getroot()
@@ -141,7 +141,7 @@
                 descriptionDict = None
                 processThisSample = False
                 reason = 'No description'
-            print ('Processing sample from ' + location + ' on ' + datetext)
+            print(('Processing sample from ' + location + ' on ' + datetext))
             #create null sample dict
             sampleDict = {}
             sampleMetaDict = {}
@@ -229,13 +229,13 @@
                                     sampleMetaDict[characteristic] = {'samplefraction':samplefraction, 'units':units, 'pcode':pcode, 'quality':quality, 'count':count}
                     #end results loop
                     except etree.XMLSyntaxError as detail:
-                        print "File contains invalid XML syntax: ", detail   
+                        print("File contains invalid XML syntax: ", detail)   
                         processThisSample = False
                         reason = "Entry contains invalid XML syntax."
             #check whether sample has all the required constituents
 #            print "Checking for requirements."
             if (processThisSample):                
-                for characteristic in charDict.iterkeys():
+                for characteristic in charDict.keys():
                     if (charDict[characteristic]['IsRequired'] != '0'):
                         if not(characteristic in sampleDict):
                             processThisSample = False
@@ -247,7 +247,7 @@
                     try:
                         os.makedirs(sampledir)
                     except os.error:
-                        print ("Problem creating location directory: " + sampledir)
+                        print(("Problem creating location directory: " + sampledir))
                         processThisSample = False
                         reason = "Problem creating location directory: " + sampledir
                 
@@ -280,13 +280,13 @@
                 #Create Panel to contain sample meta data                
                 samplePanelRow = Panel({
                         'data':DataFrame(sampleDict, index=[rowdate], dtype='float'),
-                        'time':DataFrame(descriptionDict['time'],index=[rowdate], columns=sampleMetaDict.keys()), 
-                        'timezone':DataFrame(descriptionDict['timezone'],index=[rowdate], columns=sampleMetaDict.keys()), 
-                        'pcode':DataFrame([extractValues(sampleMetaDict, ['pcode'])['values']], index=[rowdate], columns=sampleMetaDict.keys()),
-                        'quality':DataFrame([extractValues(sampleMetaDict, ['quality'])['values']], index=[rowdate], columns=sampleMetaDict.keys()),
-                        'fraction':DataFrame([extractValues(sampleMetaDict, ['samplefraction'])['values']], index=[rowdate], columns=sampleMetaDict.keys()),
-                        'units':DataFrame([extractValues(sampleMetaDict, ['units'])['values']], index=[rowdate], columns=sampleMetaDict.keys()),
-                        'count':DataFrame([extractValues(sampleMetaDict, ['count'])['values']], index=[rowdate], columns=sampleMetaDict.keys()),
+                        'time':DataFrame(descriptionDict['time'],index=[rowdate], columns=list(sampleMetaDict.keys())), 
+                        'timezone':DataFrame(descriptionDict['timezone'],index=[rowdate], columns=list(sampleMetaDict.keys())), 
+                        'pcode':DataFrame([extractValues(sampleMetaDict, ['pcode'])['values']], index=[rowdate], columns=list(sampleMetaDict.keys())),
+                        'quality':DataFrame([extractValues(sampleMetaDict, ['quality'])['values']], index=[rowdate], columns=list(sampleMetaDict.keys())),
+                        'fraction':DataFrame([extractValues(sampleMetaDict, ['samplefraction'])['values']], index=[rowdate], columns=list(sampleMetaDict.keys())),
+                        'units':DataFrame([extractValues(sampleMetaDict, ['units'])['values']], index=[rowdate], columns=list(sampleMetaDict.keys())),
+                        'count':DataFrame([extractValues(sampleMetaDict, ['count'])['values']], index=[rowdate], columns=list(sampleMetaDict.keys())),
                         })
                 #sampleMetaRow = Series(sampleMetaDict, index=[to_datetime(datetext)], dtype='object')
                 #Previous solution was reading/writing from pickle files
@@ -306,12 +306,12 @@
                 samples_processed.append(location + ' ' + datetext)
             else:
                 samples_not_processed.append(location + ' ' + datetext + ' - ' + reason)
-        print('Number of Samples Processed = '+ str(len(samples_processed)))
-        print('Number of Samples Not Processed = ' + str(len(samples_not_processed)))
+        print(('Number of Samples Processed = '+ str(len(samples_processed))))
+        print(('Number of Samples Not Processed = ' + str(len(samples_not_processed))))
         
         #Write out individual site data pickle and csv files in each site directory
         print('Writing out site data files...')
-        for location, pnl in sitesDict.iteritems():
+        for location, pnl in sitesDict.items():
             print(location)
             pickleFile =  os.path.join(sitesdir, location, location + '-Panel.pkl')
             pickle.dump(pnl, open(pickleFile, 'wb'))
@@ -324,7 +324,7 @@
         #Process sites through PHREEQC
         if RUN_PHREEQC:
             print("Processing site water chemisty data in PHREEQC...")
-            for location, pnl in sitesDict.iteritems():               
+            for location, pnl in sitesDict.items():               
                 phreeqc_df = processPanel(pnl, os.path.join(sitesdir,location), PHREEQC_PATH, DATABASE_FILE)  
                 phreeqc_site_file = os.path.join(sitesdir,location,location+'-PHREEQC.pkl')
                 try:
@@ -333,7 +333,7 @@
                 except IOError:
                     print('Problem writing out PHREEQC data file.')
             if bracket_charge_balance:
-                for location, pnl in sitesDict.iteritems():               
+                for location, pnl in sitesDict.items():               
                     #Force balance on Calcium
                     phreeqc_df_ca = processPanel(pnl, os.path.join(sitesdir,location), PHREEQC_PATH, DATABASE_FILE, force_balance='Ca')               
                     phreeqc_site_file_ca = os.path.join(sitesdir,location,location+'-PHREEQC-Ca.pkl')
@@ -351,40 +351,40 @@
                     except IOError:
                         print('Problem writing out PHREEQC Alk data file.')                
         #Create log file
-        print('Writing log file: '+LOG_FILE+splittag)
+        print(('Writing log file: '+LOG_FILE+splittag))
         try: 
             log_file = open(LOG_FILE+splittag, 'w')
-            print >>log_file, 'Start file = ' + START_FILE
-            print >>log_file, 'Number of Samples Processed = '+ str(len(samples_processed))
-            print >>log_file, 'Number of Samples Not Processed = ' + str(len(samples_not_processed))
-            print >>log_file, "###############"
-            print >>log_file, "Characteristics"
-            print >>log_file, "###############"
+            print('Start file = ' + START_FILE, file=log_file)
+            print('Number of Samples Processed = '+ str(len(samples_processed)), file=log_file)
+            print('Number of Samples Not Processed = ' + str(len(samples_not_processed)), file=log_file)
+            print("###############", file=log_file)
+            print("Characteristics", file=log_file)
+            print("###############", file=log_file)
             printColumnNames = True
-            for key, flags in charDict.iteritems():
+            for key, flags in charDict.items():
                 if (printColumnNames):
                     names = ['characteristic']# + '\t'
-                    for column in flags.iterkeys():
+                    for column in flags.keys():
                         names.append(str(column))
-                    print >>log_file, str("\t".join(names))
+                    print(str("\t".join(names)), file=log_file)
                     printColumnNames = False
                 columns = [key]
-                for column in flags.iterkeys():
-                    if isinstance(flags[column], basestring):
+                for column in flags.keys():
+                    if isinstance(flags[column], str):
                         columns.append(flags[column])                
-                print >>log_file, str("\t".join(columns))
-            print >>log_file, "###############"
-            print >>log_file, "Samples processed"
-            print >>log_file, "###############"
+                print(str("\t".join(columns)), file=log_file)
+            print("###############", file=log_file)
+            print("Samples processed", file=log_file)
+            print("###############", file=log_file)
             for line in samples_processed:
-                print >>log_file, line
-            print >>log_file, "###############"
-            print >>log_file, "Samples not processed"
-            print >>log_file, "###############"
+                print(line, file=log_file)
+            print("###############", file=log_file)
+            print("Samples not processed", file=log_file)
+            print("###############", file=log_file)
             for line in samples_not_processed:
-                print >>log_file, line                
+                print(line, file=log_file)                
         except IOError:
-            print("Problem opening log file: " + LOG_FILE)
+            print(("Problem opening log file: " + LOG_FILE))
             return -1
     #exceptions for parsing of xml file
     except IOError:
@@ -392,9 +392,9 @@
         #Note: can throw this error when discharge values are not read correctly,
         #I should fix this, 6/16/2014
     except etree.XMLSyntaxError as detail:
-        print "File contains invalid XML syntax: ", detail
+        print("File contains invalid XML syntax: ", detail)
     except requests.exceptions.RequestException as detail:
-        print "Error retrieving data by xml query: ", detail 
+        print("Error retrieving data by xml query: ", detail) 
     return 0
 
 
@@ -424,9 +424,9 @@
     num_samples = 0
     num_processed = 0
     if not(type(autosplitnum)==int):
-        print "autosplitnum must be an integer."
+        print("autosplitnum must be an integer.")
         return -1
-    print('Processing: '+ startfilename)
+    print(('Processing: '+ startfilename))
     try:
         #open start file
         startfile = xlrd.open_workbook(startfilename)
@@ -446,7 +446,7 @@
                         column_headings = line[1:]
                         characteristicsBlockStarted = True
                 else: #we are in the characteristics block
-                    charDict[line[0]] = dict(zip(column_headings,line[1:])) 
+                    charDict[line[0]] = dict(list(zip(column_headings,line[1:]))) 
         DATABASE_FILE = os.path.join(
             settingsDict['Path to chemical database'], 
             settingsDict['Name of chemical database'])
@@ -465,8 +465,8 @@
                 settingsDict['Input file'])
             xml_list = glob(xml_file_string)
             if xml_list==[]:
-                print "Empty xml file list. Check path for xml file."
-                print "xml file string =", xml_file_string
+                print("Empty xml file list. Check path for xml file.")
+                print("xml file string =", xml_file_string)
                 return -1
             n_xml = len(xml_list)
             if n_xml>1:
@@ -503,11 +503,11 @@
             try:
                 siteList = extractSitesFromXML(settingsDict['Input file'])
             except IOError:
-                print "Problem extracting sites from XML file "+settingsDict['Input file']+" check to see if file name is correct and file is in right location."
+                print("Problem extracting sites from XML file "+settingsDict['Input file']+" check to see if file name is correct and file is in right location.")
                 return -1
             charList = []
             #collect list of characteristics to query
-            for key in charDict.iterkeys():
+            for key in charDict.keys():
                 charList.append(str(key))
             if len(siteList)>autosplitnum:
                 #We have too long of a list and should split into multiple queries
@@ -552,12 +552,12 @@
             try:
                 siteList = extractSitesFromText(settingsDict['Input file'])
             except IOError:
-                print "Problem extracting sites from text file "+settingsDict['Input file']+" check to see if file name is correct and file is in right location."
+                print("Problem extracting sites from text file "+settingsDict['Input file']+" check to see if file name is correct and file is in right location.")
                 return -1
             if (siteList != -1):
                 charList = []
                 #collect list of characteristics to query
-                for key in charDict.iterkeys():
+                for key in charDict.keys():
                     charList.append(str(key))
                 if len(siteList)>autosplitnum:
                     #We have too long of a list and should split into multiple queries
@@ -598,7 +598,7 @@
             else:
                 print("Problem obtaining site list.")
         else:
-            print('Problem with "Input Method" of start file: ' + settingsDict['Input method'])
+            print(('Problem with "Input Method" of start file: ' + settingsDict['Input method']))
 
     except IOError:
         print("Problem reading start file.  Check file name.")
--- olm/USGS/dataSlice.py	(original)
+++ olm/USGS/dataSlice.py	(refactored)
@@ -21,7 +21,7 @@
         iterDict = tempIterDict
     values = []
     gaps = []
-    for subDictKey in iterDict.iterkeys():
+    for subDictKey in iterDict.keys():
         subDict = iterDict[subDictKey]
         temp = subDict
         isGap = False
@@ -52,7 +52,7 @@
     for list in enumerate(dataKey2DList):
         valuesList.append([])
         gapsList.append([])
-    for subDictKey in iterDict.iterkeys():
+    for subDictKey in iterDict.keys():
         subDict = iterDict[subDictKey]
         isGap = False
         variableList = []
--- olm/USGS/loadWaterQualityData.py	(original)
+++ olm/USGS/loadWaterQualityData.py	(refactored)
@@ -2,9 +2,9 @@
 Functions to load water quality data that has been processed and pickled by WQXtoPHREEQC
 """
 import os
-import cPickle as pickle
+import pickle as pickle
 from pandas.io.pickle import read_pickle
-from siteListExtraction import *
+from .siteListExtraction import *
 from glob import glob
 
 DEFAULT_DIR = './Processed-Sites'
@@ -68,14 +68,14 @@
         siteList = siteListFromRegEx(regEx, processedSitesDir = processedSitesDir)
     else:
         # if not provided then query user for needed data
-        processedSitesInput = raw_input("Path of the processed sites directory (Default = ./Processed-Sites): ")
+        processedSitesInput = input("Path of the processed sites directory (Default = ./Processed-Sites): ")
         if (processedSitesInput != ''):
             processedSitesDir = processedSitesInput
         print(processedSitesDir)
         processedSitesDir = checkSitesDir(processedSitesDir)
         modeOK = False
         while not(modeOK):
-            mode = raw_input("Do you want to: \n \t 1) enter a semi-colon separated list of sites \n \t 2) provide a text file of sites \n \t \n\t 3) provide an XML list of sites \n \t 4) provide a wildcard expression to obtain sites from directory list\n Enter 1, 2, 3, or 4: ")
+            mode = input("Do you want to: \n \t 1) enter a semi-colon separated list of sites \n \t 2) provide a text file of sites \n \t \n\t 3) provide an XML list of sites \n \t 4) provide a wildcard expression to obtain sites from directory list\n Enter 1, 2, 3, or 4: ")
             if mode.isdigit():
                 if ( (int(mode) > 0) and (int(mode) < 5) ):
                     modeOK = True
@@ -84,16 +84,16 @@
             else:
                 print("Invalid input")
         if (int(mode) == 1):
-            siteListText = raw_input("Enter list of sites separated by semi-colons: ")
+            siteListText = input("Enter list of sites separated by semi-colons: ")
             siteList = siteListFromLine(siteListText)
         elif (int(mode) == 2):
-            siteFile = raw_input("Enter path to text file containing site list: ")
+            siteFile = input("Enter path to text file containing site list: ")
             siteList = siteListFromFile(siteFile)
         elif (int(mode) == 3):
-            siteFile = raw_input("Enter path to XML file containing site list: ")
+            siteFile = input("Enter path to XML file containing site list: ")
             siteList = siteListFromFile(siteFile, XML=True)
         elif (int(mode) == 4):
-            regEx = raw_input("Enter regular expression: ")
+            regEx = input("Enter regular expression: ")
             siteList = siteListFromRegEx(regEx)
     if (siteList != -1):
         #process the sites in the list
@@ -129,7 +129,7 @@
         metaDataFile = os.path.join(processedSitesDir, site, site+'-Site-Description.pkl')
         siteMetaData = pickle.load(open(metaDataFile, 'rb'))
     except IOError:
-        print ("Problem reading pickle file: " + panelFile )
+        print(("Problem reading pickle file: " + panelFile ))
         return None
     return siteMetaData
     
@@ -161,7 +161,7 @@
 #        sitePanel = pickle.load(open(panelFile, 'rb'))
         sitePanel = read_pickle(panelFile)
     except IOError:
-        print ("Problem reading pickle file: " + panelFile )
+        print(("Problem reading pickle file: " + panelFile ))
         return None
     return sitePanel
 
@@ -190,7 +190,7 @@
         phreeqcFile = os.path.join(processedSitesDir, site, site+'-PHREEQC.pkl')
         sitedf = read_pickle(phreeqcFile)
     except IOError:
-        print ("Problem reading pickle file: " + phreeqcFile )
+        print(("Problem reading pickle file: " + phreeqcFile ))
         return None
     return sitedf
             
@@ -231,6 +231,6 @@
 def checkSitesDir(processedSitesDir):
     while not os.path.exists(processedSitesDir):
         print("Invalid path to processed sites directory.")
-        processedSitesDir = raw_input("Path of the processed sites directory (Default = ./Processed-Sites): ")
+        processedSitesDir = input("Path of the processed sites directory (Default = ./Processed-Sites): ")
     return processedSitesDir
 
--- olm/USGS/plots.py	(original)
+++ olm/USGS/plots.py	(refactored)
@@ -84,7 +84,7 @@
     plotsDir = os.path.join(sitesDir, 'plots/')
     #Check for plots directory and make it if one doesn't exist
     check_plots_dir(sitesDir)
-    for site in sitePanelDict.keys():
+    for site in list(sitePanelDict.keys()):
         if classFile !='':
             #read in classification for this site
             this_class = class_xls['recharge'][site]
@@ -92,7 +92,7 @@
             site_class.append(this_class)
         else:
             this_color='black'
-        print("Making plot for: "+site)
+        print(("Making plot for: "+site))
         Q = sitePanelDict[site].data['Stream flow, mean. daily']
         pwp_rates = calc_site_pwp(sitePanelDict[site], sitePhreeqcDict[site])
         pwp_rates = pwp_to_mm_yr(pwp_rates)
@@ -108,9 +108,9 @@
                 k = opt_params[0]
                 a = opt_params[1]
                 beta = opt_params[2]
-            except RuntimeError, e:
+            except RuntimeError as e:
                 fit_converged = False
-                print e
+                print(e)
         else:
             fit_converged = False
         #Make Discharge vs. Saturation Ratio Plot
@@ -190,15 +190,15 @@
     arr_site_class = arr_site_class[goodids]
     if arr_rs[arr_site_class==1].shape[0]>1:
         arr1_rs_sig = arr_rs[logical_and(arr_site_class==1, arr_ps<0.05)]
-        print "Class 1: number of sig ps = ",arr1_rs_sig.size,  "number insig = ", arr_rs[arr_site_class==1].size - arr1_rs_sig.size       
+        print("Class 1: number of sig ps = ",arr1_rs_sig.size,  "number insig = ", arr_rs[arr_site_class==1].size - arr1_rs_sig.size)       
         hist(arr1_rs_sig, nbins, color=class_colors[1], alpha=0.5)
     if arr_rs[arr_site_class==2].shape[0]>1:
         arr2_rs_sig = arr_rs[logical_and(arr_site_class==2, arr_ps<0.05)]
-        print "Class 2: number of sig ps = ",arr2_rs_sig.size,  "number insig = ", arr_rs[arr_site_class==2].size - arr2_rs_sig.size       
+        print("Class 2: number of sig ps = ",arr2_rs_sig.size,  "number insig = ", arr_rs[arr_site_class==2].size - arr2_rs_sig.size)       
         hist(arr2_rs_sig, nbins, color=class_colors[2], alpha=0.5)
     if arr_rs[arr_site_class==3].shape[0]>1:
         arr3_rs_sig = arr_rs[logical_and(arr_site_class==3, arr_ps<0.05)]
-        print "Class 3: number of sig ps = ",arr3_rs_sig.size,  "number insig = ", arr_rs[arr_site_class==3].size - arr3_rs_sig.size       
+        print("Class 3: number of sig ps = ",arr3_rs_sig.size,  "number insig = ", arr_rs[arr_site_class==3].size - arr3_rs_sig.size)       
         hist(arr3_rs_sig, nbins, color=class_colors[3], alpha=0.5)
     xlabel('Pearson R')
     ylabel('Frequency')
@@ -226,15 +226,15 @@
     figure()
     if arr_spr[arr_site_class==1].shape[0]>1:
         arr1_spr_sig = arr_spr[logical_and(arr_site_class==1, arr_spp<0.05)]
-        print "Class 1: number of sig ps = ",arr1_spr_sig.size,  "number insig = ", arr_spr[arr_site_class==1].size - arr1_spr_sig.size       
+        print("Class 1: number of sig ps = ",arr1_spr_sig.size,  "number insig = ", arr_spr[arr_site_class==1].size - arr1_spr_sig.size)       
         hist(arr1_spr_sig, nbins, color=class_colors[1], alpha=0.5)
     if arr_spr[arr_site_class==2].shape[0]>1:
         arr2_spr_sig = arr_spr[logical_and(arr_site_class==2, arr_spp<0.05)]
-        print "Class 2: number of sig ps = ",arr2_spr_sig.size,  "number insig = ", arr_spr[arr_site_class==2].size - arr2_spr_sig.size       
+        print("Class 2: number of sig ps = ",arr2_spr_sig.size,  "number insig = ", arr_spr[arr_site_class==2].size - arr2_spr_sig.size)       
         hist(arr2_spr_sig, nbins, color=class_colors[2], alpha=0.5)
     if arr_spr[arr_site_class==3].shape[0]>1:
         arr3_spr_sig = arr_spr[logical_and(arr_site_class==3, arr_spp<0.05)]
-        print "Class 3: number of sig ps = ",arr3_spr_sig.size,  "number insig = ", arr_spr[arr_site_class==3].size - arr3_spr_sig.size       
+        print("Class 3: number of sig ps = ",arr3_spr_sig.size,  "number insig = ", arr_spr[arr_site_class==3].size - arr3_spr_sig.size)       
         hist(arr3_spr_sig, nbins, color=class_colors[3], alpha=0.5)
     xlabel('Spearman R')
     ylabel('Frequency')
@@ -264,7 +264,7 @@
     plotsDir = os.path.join(sitesDir, 'plots/')
     #Check for plots directory and make it if one doesn't exist
     check_plots_dir(sitesDir)
-    for site in sitePanelDict.keys():
+    for site in list(sitePanelDict.keys()):
         if classFile !='':
             #read in classification for this site
             this_class = class_xls['recharge'][site]
@@ -272,7 +272,7 @@
             site_class.append(this_class)
         else:
             this_color='black'
-        print("Making plot for: "+site)
+        print(("Making plot for: "+site))
         Q = sitePanelDict[site].data['Stream flow, mean. daily']
         pwp_rates = calc_site_pwp(sitePanelDict[site], sitePhreeqcDict[site])
         pwp_rates = pwp_to_mm_yr(pwp_rates)
@@ -306,7 +306,7 @@
     #Make histogram of pearson r values    
     figure()
     arr_rs = array(rs)
-    print "size arr_rs=", arr_rs.size
+    print("size arr_rs=", arr_rs.size)
     arr_ps = array(ps)
     arr_spr = array(spr)
     arr_spp = array(spp)
@@ -352,14 +352,14 @@
     ps = []
     slopes = []
     plotsDir = os.path.join(sitesDir, 'plots/')
-    for site in sitePanelDict.keys():
+    for site in list(sitePanelDict.keys()):
         if classFile !='':
             #read in classification for this site
             this_class = class_xls['recharge'][site]
             this_color=class_colors[this_class]
         else:
             this_color='black'
-        print("Making plot for: "+site)
+        print(("Making plot for: "+site))
         #Read out data into shorter variable names
         Q = sitePanelDict[site].data['Stream flow, mean. daily']
         T_C = sitePanelDict[site].data['Temperature, water']
@@ -426,8 +426,8 @@
     ps = []
     slopes = []
     plotsDir = os.path.join(sitesDir, 'plots/')
-    for site in sitePanelDict.keys():
-        print("Making plot for: "+site)
+    for site in list(sitePanelDict.keys()):
+        print(("Making plot for: "+site))
         #Read out data into shorter variable names
         Q = sitePanelDict[site].data['Stream flow, mean. daily']
         SI = sitePhreeqcDict[site].SI_Calcite
@@ -467,8 +467,8 @@
     ps = []
     slopes = []
     plotsDir = os.path.join(sitesDir, 'plots/')
-    for site in sitePanelDict.keys():
-        print("Making plot for: "+site)
+    for site in list(sitePanelDict.keys()):
+        print(("Making plot for: "+site))
         #Read out data into shorter variable names
         Q = sitePanelDict[site].data['Stream flow, mean. daily']
         T_C = sitePanelDict[site].data['Temperature, water']
@@ -529,7 +529,7 @@
     plotsDir = os.path.join(sitesDir, 'plots/')
     #Check for plots directory and make it if one doesn't exist
     check_plots_dir(sitesDir)
-    for site in sitePanelDict.keys():
+    for site in list(sitePanelDict.keys()):
         if classFile !='':
             #read in classification for this site
             this_class = class_xls['recharge'][site]
@@ -537,7 +537,7 @@
             site_class.append(this_class)
         else:
             this_color='black'
-        print("Making plot for: "+site)
+        print(("Making plot for: "+site))
         Q = sitePanelDict[site].data['Stream flow, mean. daily']
         pwp_rates,PCO2 = calc_site_pwp(sitePanelDict[site], sitePhreeqcDict[site], returnPCO2=True)
         pwp_rates = pwp_to_mm_yr(pwp_rates)
@@ -684,7 +684,7 @@
     site_class = []
     slopes = []
     plotsDir = os.path.join(sitesDir, 'plots/')
-    for site in sitePanelDict.keys():
+    for site in list(sitePanelDict.keys()):
         if classFile !='':
             #read in classification for this site
             this_class = class_xls['recharge'][site]
@@ -692,7 +692,7 @@
             site_class.append(this_class)
         else:
             this_color='black'
-        print("Making plot for: "+site)
+        print(("Making plot for: "+site))
         #Read out data into shorter variable names
         Q = sitePanelDict[site].data['Stream flow, mean. daily']
         T_C = sitePanelDict[site].data['Temperature, water']
@@ -740,7 +740,7 @@
         class_xls = read_excel(classFile, 'Sheet1',index_col=1, names=['name','site', 'recharge', 'age'])
     site_class = []
     plotsDir = os.path.join(sitesDir, 'plots/')
-    for site in sitePanelDict.keys():
+    for site in list(sitePanelDict.keys()):
         if classFile !='':
             #read in classification for this site
             this_class = class_xls['recharge'][site]
@@ -748,7 +748,7 @@
             site_class.append(this_class)
         else:
             this_color='black'
-        print("Making plot for: "+site)
+        print(("Making plot for: "+site))
         #Read out data into shorter variable names
         T_C = sitePanelDict[site].data['Temperature, water']
         CO2 = sitePhreeqcDict[site].CO2_Molality
@@ -771,7 +771,7 @@
         class_xls = read_excel(classFile, 'Sheet1',index_col=1, names=['name','site', 'recharge', 'age'])
     site_class = []
     plotsDir = os.path.join(sitesDir, 'plots/')
-    for site in sitePanelDict.keys():
+    for site in list(sitePanelDict.keys()):
         if classFile !='':
             #read in classification for this site
             this_class = class_xls['recharge'][site]
@@ -779,7 +779,7 @@
             site_class.append(this_class)
         else:
             this_color='black'
-        print("Making plot for: "+site)
+        print(("Making plot for: "+site))
         #Read out data into shorter variable names
         T_C = sitePanelDict[site].data['Temperature, water']
         CO2 = sitePhreeqcDict[site].CO2_Molality
--- olm/USGS/siteListExtraction.py	(original)
+++ olm/USGS/siteListExtraction.py	(refactored)
@@ -13,10 +13,10 @@
             siteList.append(siteNum)
         return siteList
     except IOError:
-        print("Error opening file: " + xmlFile)
+        print(("Error opening file: " + xmlFile))
         return -1
     except etree.XMLSyntaxError:
-        print ("File contains invalid XML Syntax: " + xmlFile)
+        print(("File contains invalid XML Syntax: " + xmlFile))
         return -1
 
 def extractSitesFromText(textFile):
--- olm/loggers/HoboToolkit.py	(original)
+++ olm/loggers/HoboToolkit.py	(refactored)
@@ -75,8 +75,8 @@
             rename_dict[label]=new_name
     #If there is only one conductivity column, we'll label it as 'Cond'
     if cond_count==1:
-        old_names = rename_dict.keys()
-        for old_name,new_name in rename_dict.iteritems():
+        old_names = list(rename_dict.keys())
+        for old_name,new_name in rename_dict.items():
             if 'Cond' in new_name:
                 cond_key = old_name
         rename_dict[cond_key] = 'Cond'
@@ -84,7 +84,7 @@
     if not(all_columns):
         #Trim out unwanted columns
         s_dict = {}
-        for col in rename_dict.itervalues():
+        for col in rename_dict.values():
             s = df[col]
             s_dict[col] = s
         df = DataFrame(s_dict)
--- olm/loggers/SchlumbergerCTDToolkit.py	(original)
+++ olm/loggers/SchlumbergerCTDToolkit.py	(refactored)
@@ -107,7 +107,7 @@
         if len(offset_list) > 0:
             #offset each data file by the value in offset list
             if len(offset_list) != len(dflist) - 1:
-                print "Number of elements in offset_list must be one less than number of data files to concatenate"
+                print("Number of elements in offset_list must be one less than number of data files to concatenate")
                 return None
             else:
                 for i, df in enumerate(dflist):
--- olm/loggers/TruBluToolkit.py	(original)
+++ olm/loggers/TruBluToolkit.py	(refactored)
@@ -40,8 +40,8 @@
             df = read_csv(csvfile, sep=sep, skiprows=skiprows, header=header, index_col=index_col, parse_dates=parse_dates)
             return df
         else:
-            print(csvfile + " is empty")
+            print((csvfile + " is empty"))
     except OSError:
-        print(csvfile + " does not exist")
+        print((csvfile + " does not exist"))
 
 
--- olm/loggers/loggerScripts.py	(original)
+++ olm/loggers/loggerScripts.py	(refactored)
@@ -34,7 +34,7 @@
                 joined[col] = filled_col
         return joined
     else:
-        print "Problem with input list: Need to input a list of DataFrame objects"
+        print("Problem with input list: Need to input a list of DataFrame objects")
         return None
 
 def joinAndResampleLoggers(loggerlist, interval, suffixes=[], how='inner', interpolate=False, limit=None):
@@ -76,17 +76,17 @@
             resampledList.append(logger.resample(interval))
     elif type(loggerlist)==dict:
         #print "Processing dict type loggerlist..."
-        for logger_key in loggerlist.keys():            
+        for logger_key in list(loggerlist.keys()):            
             logger = loggerlist[logger_key]
             if type(suffixes)==dict:
                 if suffixes[logger_key]!=None:
                     logger.columns+='_'+suffixes[logger_key]
                 resampledList.append(logger.resample(interval))
             else:
-                print "Problem with suffixes. If loggerlist is a dict, suffixes also must be a dict."
+                print("Problem with suffixes. If loggerlist is a dict, suffixes also must be a dict.")
                 return None
     else:
-        print "Problem with logger list: Need to input a list or dict of DataFrame or Series objects"
+        print("Problem with logger list: Need to input a list or dict of DataFrame or Series objects")
         return None
             
     for i, logger in enumerate(resampledList):
@@ -125,7 +125,7 @@
     """
     #loop through correction series and calculate multiplying factors
     corrDict = {}
-    for date, measurement in correctionSeries.iteritems():
+    for date, measurement in correctionSeries.items():
         candidates = rawSeries.index[notnull(rawSeries)]
         index = candidates.searchsorted(date)
         if index > 0:
